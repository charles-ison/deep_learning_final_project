{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "Dl19Asqk79-G"
      },
      "outputs": [],
      "source": [
        "!pip -qqq install transformers datasets nnAudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "KFbW0b6v7uAX"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2FeatureExtractor\n",
        "from transformers import AutoModel\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchaudio.transforms as T\n",
        "from datasets import Dataset, Audio, concatenate_datasets, Split\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXOmbPCXPgTP",
        "outputId": "a92c5a2a-5be2-4c18-9253-8296de62930b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['train', 'test']"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ],
      "source": [
        "# mount drive and set path to dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_dir = \"/content/drive/Shareddrives/DeepLearningProject/minibabyslakh\"\n",
        "# make sure \n",
        "os.listdir(data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "pxhU5aIY8KRL"
      },
      "outputs": [],
      "source": [
        "# loading our model weights\n",
        "model = AutoModel.from_pretrained(\"m-a-p/MERT-v0\", trust_remote_code=True)\n",
        "# loading the corresponding preprocessor config\n",
        "processor = Wav2Vec2FeatureExtractor.from_pretrained(\"m-a-p/MERT-v0\",trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "guN3LtRXYhuk"
      },
      "outputs": [],
      "source": [
        "# # load demo audio and set processor\n",
        "# dataset = Dataset.load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n",
        "# dataset = dataset.sort(\"id\")\n",
        "# sampling_rate = dataset.features[\"audio\"].sampling_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "-SCGvvx9JHtw"
      },
      "outputs": [],
      "source": [
        "# Function to load the audio files from the directory structure\n",
        "def get_data_files(directory):\n",
        "    bass_files = []\n",
        "    residual_files = []\n",
        "    tracks = []\n",
        "    for track_dir in os.listdir(directory):\n",
        "        track_path = os.path.join(directory, track_dir)\n",
        "        if os.path.isdir(track_path):\n",
        "            bass_audio_dir = os.path.join(track_path, 'bass')\n",
        "            # bass_file = os.path.join(bass_audio_dir, 'bass.wav')\n",
        "            # residual_file = os.path.join(bass_audio_dir, 'residuals.wav')\n",
        "            if os.path.isdir(bass_audio_dir):\n",
        "                for file in os.listdir(bass_audio_dir):\n",
        "                    if file.startswith('bass') and file.endswith('.wav'):\n",
        "                        bass_file = os.path.join(bass_audio_dir, file)\n",
        "                        bass_files.append(bass_file)\n",
        "                        residual_file = os.path.join(bass_audio_dir, 'residuals' + file[4:])\n",
        "                        residual_files.append(residual_file)\n",
        "                        tracks.append(track_dir)\n",
        "        \n",
        "    return {\"bass\": bass_files, \"residuals\": residual_files, \"track\": tracks}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0rXcxIlS1yK",
        "outputId": "dfef94c9-d958-40e7-b046-e3c8c60c2385"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bass': ['/content/drive/Shareddrives/DeepLearningProject/minibabyslakh/train/Track00002/bass/bass.wav',\n",
              "  '/content/drive/Shareddrives/DeepLearningProject/minibabyslakh/train/Track00001/bass/bass.wav',\n",
              "  '/content/drive/Shareddrives/DeepLearningProject/minibabyslakh/train/Track00003/bass/bass.wav'],\n",
              " 'residuals': ['/content/drive/Shareddrives/DeepLearningProject/minibabyslakh/train/Track00002/bass/residuals.wav',\n",
              "  '/content/drive/Shareddrives/DeepLearningProject/minibabyslakh/train/Track00001/bass/residuals.wav',\n",
              "  '/content/drive/Shareddrives/DeepLearningProject/minibabyslakh/train/Track00003/bass/residuals.wav'],\n",
              " 'track': ['Track00002', 'Track00001', 'Track00003']}"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ],
      "source": [
        "# Get the audio filenames from the dataset directory\n",
        "train_files = get_data_files(os.path.join(data_dir, \"train\"))\n",
        "test_files = get_data_files(os.path.join(data_dir, \"test\"))\n",
        "# validation_data = load_audio_files(os.path.join(data_dir, \"validation\"))\n",
        "train_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4tfjuUfYpCV",
        "outputId": "94705122-4294-4d80-e024-8aa03c7c7cd4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['bass', 'residuals', 'track'],\n",
              "    num_rows: 3\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ],
      "source": [
        "# Create the dataset objects\n",
        "train_dataset = Dataset.from_dict(train_files, split=\"train\") \\\n",
        "                    .cast_column(\"bass\", Audio()) \\\n",
        "                    .cast_column(\"residuals\", Audio()) \\\n",
        "                    .sort(\"track\")\n",
        "test_dataset = Dataset.from_dict(test_files, split=\"test\") \\\n",
        "                    .cast_column(\"bass\", Audio()) \\\n",
        "                    .cast_column(\"residuals\", Audio()) \\\n",
        "                    .sort(\"track\")\n",
        "combined_dataset = concatenate_datasets([train_dataset, test_dataset])\n",
        "\n",
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "3kxvhTbB-IKA"
      },
      "outputs": [],
      "source": [
        "sampling_rate = train_dataset[\"residuals\"][0]['sampling_rate']\n",
        "resample_rate = processor.sampling_rate\n",
        "# make sure the sample_rate aligned\n",
        "if resample_rate != sampling_rate:\n",
        "    print(f'setting rate from {sampling_rate} to {resample_rate}')\n",
        "    resampler = T.Resample(sampling_rate, resample_rate)\n",
        "else:\n",
        "    resampler = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "KIn1GRQa-wx-"
      },
      "outputs": [],
      "source": [
        "# audio file is decoded on the fly\n",
        "if resampler is None:\n",
        "    src_audio = train_dataset[0][\"residuals\"][\"array\"]\n",
        "    tgt_audio = train_dataset[0][\"bass\"][\"array\"]\n",
        "else:\n",
        "  src_audio = resampler(torch.from_numpy(train_dataset[0][\"residuals\"][\"array\"]))\n",
        "  tgt_audio = resampler(torch.from_numpy(train_dataset[0][\"bass\"][\"array\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "cjrxz_BYiYiN"
      },
      "outputs": [],
      "source": [
        "# The whole audio file is too big to run in colab\n",
        "src_audio = src_audio[0:93680]\n",
        "tgt_audio = tgt_audio[0:93680]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "phYaHnl-_BRc"
      },
      "outputs": [],
      "source": [
        "src_inputs = processor(src_audio, sampling_rate=resample_rate, return_tensors=\"pt\")\n",
        "tgt_inputs = processor(tgt_audio, sampling_rate=resample_rate, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    src_outputs = model(**src_inputs, output_hidden_states=True)\n",
        "    tgt_outputs = model(**tgt_inputs, output_hidden_states=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhaB1K8-iJXR",
        "outputId": "c3a297b9-2757-47ef-a23a-d2811e0392ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src_hidden_states.shape: torch.Size([13, 292, 768])\n",
            "tgt_hidden_states.shape: torch.Size([13, 292, 768])\n"
          ]
        }
      ],
      "source": [
        "# take a look at the output shape, there are 13 layers of representation\n",
        "# each layer performs differently in different downstream tasks, you should choose empirically\n",
        "src_hidden_states = torch.stack(src_outputs.hidden_states).squeeze()\n",
        "tgt_hidden_states = torch.stack(tgt_outputs.hidden_states).squeeze()\n",
        "\n",
        "print(\"src_hidden_states.shape: \" + str(src_hidden_states.shape))\n",
        "print(\"tgt_hidden_states.shape: \" + str(tgt_hidden_states.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define Model"
      ],
      "metadata": {
        "id": "7nHJRvijLxVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = src_hidden_states.size(dim=1)\n",
        "d_model = src_hidden_states.size(dim=2)\n",
        "\n",
        "transformer_decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=8)\n",
        "transformer_decoder = nn.TransformerDecoder(transformer_decoder_layer, num_layers=6)\n",
        "\n",
        "print(transformer_decoder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSn9CPlMBcVH",
        "outputId": "f2a9b8ba-c690-46e5-a3f5-842559bc1a56"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TransformerDecoder(\n",
            "  (layers): ModuleList(\n",
            "    (0-5): 6 x TransformerDecoderLayer(\n",
            "      (self_attn): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "      )\n",
            "      (multihead_attn): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "      )\n",
            "      (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
            "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout1): Dropout(p=0.1, inplace=False)\n",
            "      (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      (dropout3): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Model"
      ],
      "metadata": {
        "id": "9eJULPtnL0r_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nj0xTETLpxtx",
        "outputId": "dcdff2e3-80ac-4e51-ea04-2013514dfecc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating token: 0\n",
            "Generating token: 1\n",
            "Generating token: 2\n",
            "Generating token: 3\n",
            "Generating token: 4\n",
            "Generating token: 5\n",
            "Generating token: 6\n",
            "Generating token: 7\n",
            "Generating token: 8\n",
            "Generating token: 9\n",
            "Generating token: 10\n",
            "Generating token: 11\n",
            "Generating token: 12\n",
            "Generating token: 13\n",
            "Generating token: 14\n",
            "Generating token: 15\n",
            "Generating token: 16\n",
            "Generating token: 17\n",
            "Generating token: 18\n",
            "Generating token: 19\n",
            "Generating token: 20\n",
            "Generating token: 21\n",
            "Generating token: 22\n",
            "Generating token: 23\n",
            "Generating token: 24\n",
            "Generating token: 25\n",
            "Generating token: 26\n",
            "Generating token: 27\n",
            "Generating token: 28\n",
            "Generating token: 29\n",
            "Generating token: 30\n",
            "Generating token: 31\n",
            "Generating token: 32\n",
            "Generating token: 33\n",
            "Generating token: 34\n",
            "Generating token: 35\n",
            "Generating token: 36\n",
            "Generating token: 37\n",
            "Generating token: 38\n",
            "Generating token: 39\n",
            "Generating token: 40\n",
            "Generating token: 41\n",
            "Generating token: 42\n",
            "Generating token: 43\n",
            "Generating token: 44\n",
            "Generating token: 45\n",
            "Generating token: 46\n",
            "Generating token: 47\n",
            "Generating token: 48\n",
            "Generating token: 49\n",
            "Generating token: 50\n",
            "Generating token: 51\n",
            "Generating token: 52\n",
            "Generating token: 53\n",
            "Generating token: 54\n",
            "Generating token: 55\n",
            "Generating token: 56\n",
            "Generating token: 57\n",
            "Generating token: 58\n",
            "Generating token: 59\n",
            "Generating token: 60\n",
            "Generating token: 61\n",
            "Generating token: 62\n",
            "Generating token: 63\n",
            "Generating token: 64\n",
            "Generating token: 65\n",
            "Generating token: 66\n",
            "Generating token: 67\n",
            "Generating token: 68\n",
            "Generating token: 69\n",
            "Generating token: 70\n",
            "Generating token: 71\n",
            "Generating token: 72\n",
            "Generating token: 73\n",
            "Generating token: 74\n",
            "Generating token: 75\n",
            "Generating token: 76\n",
            "Generating token: 77\n",
            "Generating token: 78\n",
            "Generating token: 79\n",
            "Generating token: 80\n",
            "Generating token: 81\n",
            "Generating token: 82\n",
            "Generating token: 83\n",
            "Generating token: 84\n",
            "Generating token: 85\n",
            "Generating token: 86\n",
            "Generating token: 87\n",
            "Generating token: 88\n",
            "Generating token: 89\n",
            "Generating token: 90\n",
            "Generating token: 91\n",
            "Generating token: 92\n",
            "Generating token: 93\n",
            "Generating token: 94\n",
            "Generating token: 95\n",
            "Generating token: 96\n",
            "Generating token: 97\n",
            "Generating token: 98\n",
            "Generating token: 99\n",
            "Generating token: 100\n",
            "Generating token: 101\n",
            "Generating token: 102\n",
            "Generating token: 103\n",
            "Generating token: 104\n",
            "Generating token: 105\n",
            "Generating token: 106\n",
            "Generating token: 107\n",
            "Generating token: 108\n",
            "Generating token: 109\n",
            "Generating token: 110\n",
            "Generating token: 111\n",
            "Generating token: 112\n",
            "Generating token: 113\n",
            "Generating token: 114\n",
            "Generating token: 115\n",
            "Generating token: 116\n",
            "Generating token: 117\n",
            "Generating token: 118\n",
            "Generating token: 119\n",
            "Generating token: 120\n",
            "Generating token: 121\n",
            "Generating token: 122\n",
            "Generating token: 123\n",
            "Generating token: 124\n",
            "Generating token: 125\n",
            "Generating token: 126\n",
            "Generating token: 127\n",
            "Generating token: 128\n",
            "Generating token: 129\n",
            "Generating token: 130\n",
            "Generating token: 131\n",
            "Generating token: 132\n",
            "Generating token: 133\n",
            "Generating token: 134\n",
            "Generating token: 135\n",
            "Generating token: 136\n",
            "Generating token: 137\n",
            "Generating token: 138\n",
            "Generating token: 139\n",
            "Generating token: 140\n",
            "Generating token: 141\n",
            "Generating token: 142\n",
            "Generating token: 143\n",
            "Generating token: 144\n",
            "Generating token: 145\n",
            "Generating token: 146\n",
            "Generating token: 147\n",
            "Generating token: 148\n",
            "Generating token: 149\n",
            "Generating token: 150\n",
            "Generating token: 151\n",
            "Generating token: 152\n",
            "Generating token: 153\n",
            "Generating token: 154\n",
            "Generating token: 155\n",
            "Generating token: 156\n",
            "Generating token: 157\n",
            "Generating token: 158\n",
            "Generating token: 159\n",
            "Generating token: 160\n",
            "Generating token: 161\n",
            "Generating token: 162\n",
            "Generating token: 163\n",
            "Generating token: 164\n",
            "Generating token: 165\n",
            "Generating token: 166\n",
            "Generating token: 167\n",
            "Generating token: 168\n",
            "Generating token: 169\n",
            "Generating token: 170\n",
            "Generating token: 171\n",
            "Generating token: 172\n",
            "Generating token: 173\n",
            "Generating token: 174\n",
            "Generating token: 175\n",
            "Generating token: 176\n",
            "Generating token: 177\n",
            "Generating token: 178\n",
            "Generating token: 179\n",
            "Generating token: 180\n",
            "Generating token: 181\n",
            "Generating token: 182\n",
            "Generating token: 183\n",
            "Generating token: 184\n",
            "Generating token: 185\n",
            "Generating token: 186\n",
            "Generating token: 187\n",
            "Generating token: 188\n",
            "Generating token: 189\n",
            "Generating token: 190\n",
            "Generating token: 191\n",
            "Generating token: 192\n",
            "Generating token: 193\n",
            "Generating token: 194\n",
            "Generating token: 195\n",
            "Generating token: 196\n",
            "Generating token: 197\n",
            "Generating token: 198\n",
            "Generating token: 199\n",
            "Generating token: 200\n",
            "Generating token: 201\n",
            "Generating token: 202\n",
            "Generating token: 203\n",
            "Generating token: 204\n",
            "Generating token: 205\n",
            "Generating token: 206\n",
            "Generating token: 207\n",
            "Generating token: 208\n",
            "Generating token: 209\n",
            "Generating token: 210\n",
            "Generating token: 211\n",
            "Generating token: 212\n",
            "Generating token: 213\n",
            "Generating token: 214\n",
            "Generating token: 215\n",
            "Generating token: 216\n",
            "Generating token: 217\n",
            "Generating token: 218\n",
            "Generating token: 219\n",
            "Generating token: 220\n",
            "Generating token: 221\n",
            "Generating token: 222\n",
            "Generating token: 223\n",
            "Generating token: 224\n",
            "Generating token: 225\n",
            "Generating token: 226\n",
            "Generating token: 227\n",
            "Generating token: 228\n",
            "Generating token: 229\n",
            "Generating token: 230\n",
            "Generating token: 231\n",
            "Generating token: 232\n",
            "Generating token: 233\n",
            "Generating token: 234\n",
            "Generating token: 235\n",
            "Generating token: 236\n",
            "Generating token: 237\n",
            "Generating token: 238\n",
            "Generating token: 239\n",
            "Generating token: 240\n",
            "Generating token: 241\n",
            "Generating token: 242\n",
            "Generating token: 243\n",
            "Generating token: 244\n",
            "Generating token: 245\n",
            "Generating token: 246\n",
            "Generating token: 247\n",
            "Generating token: 248\n",
            "Generating token: 249\n",
            "Generating token: 250\n",
            "Generating token: 251\n",
            "Generating token: 252\n",
            "Generating token: 253\n",
            "Generating token: 254\n",
            "Generating token: 255\n",
            "Generating token: 256\n",
            "Generating token: 257\n",
            "Generating token: 258\n",
            "Generating token: 259\n",
            "Generating token: 260\n",
            "Generating token: 261\n",
            "Generating token: 262\n",
            "Generating token: 263\n",
            "Generating token: 264\n",
            "Generating token: 265\n",
            "Generating token: 266\n",
            "Generating token: 267\n",
            "Generating token: 268\n",
            "Generating token: 269\n",
            "Generating token: 270\n",
            "Generating token: 271\n",
            "Generating token: 272\n",
            "Generating token: 273\n",
            "Generating token: 274\n",
            "Generating token: 275\n",
            "Generating token: 276\n",
            "Generating token: 277\n",
            "Generating token: 278\n",
            "Generating token: 279\n",
            "Generating token: 280\n",
            "Generating token: 281\n",
            "Generating token: 282\n",
            "Generating token: 283\n",
            "Generating token: 284\n",
            "Generating token: 285\n",
            "Generating token: 286\n",
            "Generating token: 287\n",
            "Generating token: 288\n",
            "Generating token: 289\n",
            "Generating token: 290\n",
            "Generating token: 291\n"
          ]
        }
      ],
      "source": [
        "# TODO: Just taking the last layer for fine grain acoustic tokens, need to select a layer to use for semantic tokens as well and find out how to concat\n",
        "src_fine_grain_tokens = src_hidden_states[-1]\n",
        "tgt_fine_grain_tokens = tgt_hidden_states[-1]\n",
        "\n",
        "# TODO: What is the appropriate starting token?\n",
        "previously_generated_token = torch.zeros([1, d_model])\n",
        "\n",
        "generated_tokens = []\n",
        "\n",
        "for index, tgt_fine_grain_token in enumerate(tgt_fine_grain_tokens):\n",
        "  print(\"Generating token: \" + str(index))\n",
        "\n",
        "  generated_token = transformer_decoder(tgt = previously_generated_token, memory = fine_grain_tokens)\n",
        "  #TODO: Calculate loss between generated_token and tgt_fine_grain_token and then backpropogate once per loop?\n",
        "\n",
        "  generated_tokens.append(generated_token)\n",
        "  previously_generated_token = generated_token\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qUyqhwraRlLu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}