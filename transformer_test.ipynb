{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "Dl19Asqk79-G"
      },
      "outputs": [],
      "source": [
        "!pip -qqq install transformers datasets nnAudio audiolm-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "KFbW0b6v7uAX"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2FeatureExtractor\n",
        "from transformers import AutoModel\n",
        "import torch\n",
        "from torch import nn\n",
        "from audiolm_pytorch.encodec import EncodecWrapper\n",
        "import torchaudio.transforms as T\n",
        "from datasets import Dataset, Audio, concatenate_datasets, Split\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXOmbPCXPgTP",
        "outputId": "0d88548c-1e51-4dc4-b1c5-cd5f1f50493c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['train', 'test']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "# mount drive and set path to dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_dir = \"/content/drive/Shareddrives/DeepLearningProject/minibabyslakh\"\n",
        "# make sure \n",
        "os.listdir(data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "pxhU5aIY8KRL"
      },
      "outputs": [],
      "source": [
        "# loading our model weights\n",
        "model = AutoModel.from_pretrained(\"m-a-p/MERT-v0\", trust_remote_code=True)\n",
        "# loading the corresponding preprocessor config\n",
        "processor = Wav2Vec2FeatureExtractor.from_pretrained(\"m-a-p/MERT-v0\", trust_remote_code=True)\n",
        "\n",
        "# Use \"encodec = EncodecWrapper().cuda()\" when a GPU is available\n",
        "encodec = EncodecWrapper()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "guN3LtRXYhuk"
      },
      "outputs": [],
      "source": [
        "# # load demo audio and set processor\n",
        "# dataset = Dataset.load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n",
        "# dataset = dataset.sort(\"id\")\n",
        "# sampling_rate = dataset.features[\"audio\"].sampling_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "-SCGvvx9JHtw"
      },
      "outputs": [],
      "source": [
        "# Function to load the audio files from the directory structure\n",
        "def get_data_files(directory):\n",
        "    bass_files = []\n",
        "    residual_files = []\n",
        "    tracks = []\n",
        "    for track_dir in os.listdir(directory):\n",
        "        track_path = os.path.join(directory, track_dir)\n",
        "        if os.path.isdir(track_path):\n",
        "            bass_audio_dir = os.path.join(track_path, 'bass')\n",
        "            # bass_file = os.path.join(bass_audio_dir, 'bass.wav')\n",
        "            # residual_file = os.path.join(bass_audio_dir, 'residuals.wav')\n",
        "            if os.path.isdir(bass_audio_dir):\n",
        "                for file in os.listdir(bass_audio_dir):\n",
        "                    if file.startswith('bass') and file.endswith('.wav'):\n",
        "                        bass_file = os.path.join(bass_audio_dir, file)\n",
        "                        bass_files.append(bass_file)\n",
        "                        residual_file = os.path.join(bass_audio_dir, 'residuals' + file[4:])\n",
        "                        residual_files.append(residual_file)\n",
        "                        tracks.append(track_dir)\n",
        "        \n",
        "    return {\"bass\": bass_files, \"residuals\": residual_files, \"track\": tracks}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0rXcxIlS1yK",
        "outputId": "17b25421-bccb-48b5-f0ae-634cb15d3778"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bass': ['/content/drive/Shareddrives/DeepLearningProject/minibabyslakh/train/Track00002/bass/bass.wav',\n",
              "  '/content/drive/Shareddrives/DeepLearningProject/minibabyslakh/train/Track00001/bass/bass.wav',\n",
              "  '/content/drive/Shareddrives/DeepLearningProject/minibabyslakh/train/Track00003/bass/bass.wav'],\n",
              " 'residuals': ['/content/drive/Shareddrives/DeepLearningProject/minibabyslakh/train/Track00002/bass/residuals.wav',\n",
              "  '/content/drive/Shareddrives/DeepLearningProject/minibabyslakh/train/Track00001/bass/residuals.wav',\n",
              "  '/content/drive/Shareddrives/DeepLearningProject/minibabyslakh/train/Track00003/bass/residuals.wav'],\n",
              " 'track': ['Track00002', 'Track00001', 'Track00003']}"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "# Get the audio filenames from the dataset directory\n",
        "train_files = get_data_files(os.path.join(data_dir, \"train\"))\n",
        "test_files = get_data_files(os.path.join(data_dir, \"test\"))\n",
        "# validation_data = load_audio_files(os.path.join(data_dir, \"validation\"))\n",
        "train_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4tfjuUfYpCV",
        "outputId": "eb9cbe58-3b12-41fa-998f-e772ae7cfc71"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['bass', 'residuals', 'track'],\n",
              "    num_rows: 3\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "# Create the dataset objects\n",
        "train_dataset = Dataset.from_dict(train_files, split=\"train\") \\\n",
        "                    .cast_column(\"bass\", Audio()) \\\n",
        "                    .cast_column(\"residuals\", Audio()) \\\n",
        "                    .sort(\"track\")\n",
        "test_dataset = Dataset.from_dict(test_files, split=\"test\") \\\n",
        "                    .cast_column(\"bass\", Audio()) \\\n",
        "                    .cast_column(\"residuals\", Audio()) \\\n",
        "                    .sort(\"track\")\n",
        "combined_dataset = concatenate_datasets([train_dataset, test_dataset])\n",
        "\n",
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "3kxvhTbB-IKA"
      },
      "outputs": [],
      "source": [
        "sampling_rate = train_dataset[\"residuals\"][0]['sampling_rate']\n",
        "resample_rate = processor.sampling_rate\n",
        "# make sure the sample_rate aligned\n",
        "if resample_rate != sampling_rate:\n",
        "    print(f'setting rate from {sampling_rate} to {resample_rate}')\n",
        "    resampler = T.Resample(sampling_rate, resample_rate)\n",
        "else:\n",
        "    resampler = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "KIn1GRQa-wx-"
      },
      "outputs": [],
      "source": [
        "# audio file is decoded on the fly\n",
        "if resampler is None:\n",
        "    src_audio = train_dataset[0][\"residuals\"][\"array\"]\n",
        "    tgt_audio = train_dataset[0][\"bass\"][\"array\"]\n",
        "else:\n",
        "  src_audio = resampler(torch.from_numpy(train_dataset[0][\"residuals\"][\"array\"]))\n",
        "  tgt_audio = resampler(torch.from_numpy(train_dataset[0][\"bass\"][\"array\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "cjrxz_BYiYiN"
      },
      "outputs": [],
      "source": [
        "# The whole audio file is too big to run in colab\n",
        "src_audio = src_audio[0:50000]\n",
        "tgt_audio = tgt_audio[0:50000]\n",
        "\n",
        "# Required for encodec\n",
        "tgt_audio = torch.from_numpy(tgt_audio).float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "phYaHnl-_BRc"
      },
      "outputs": [],
      "source": [
        "src_inputs = processor(src_audio, sampling_rate=resample_rate, return_tensors=\"pt\")\n",
        "tgt_inputs = processor(tgt_audio, sampling_rate=resample_rate, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    src_outputs = model(**src_inputs, output_hidden_states=True)\n",
        "    encodec_tokens = encodec(tgt_audio, return_encoded = False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Figure out sequence number and number of dimensions\n",
        "print(encodec_tokens[1].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySAwLcjlkGBR",
        "outputId": "59f79dd3-10ce-40a2-8930-5ba8150ee826"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([157, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhaB1K8-iJXR",
        "outputId": "567fed5f-779f-4f5e-9f23-7dd3b7f2ce67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src_hidden_states.shape: torch.Size([13, 156, 768])\n"
          ]
        }
      ],
      "source": [
        "# take a look at the output shape, there are 13 layers of representation\n",
        "# each layer performs differently in different downstream tasks, you should choose empirically\n",
        "src_hidden_states = torch.stack(src_outputs.hidden_states).squeeze()\n",
        "print(\"src_hidden_states.shape: \" + str(src_hidden_states.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define Model"
      ],
      "metadata": {
        "id": "7nHJRvijLxVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = src_hidden_states.size(dim = 1)\n",
        "d_model = src_hidden_states.size(dim = 2)\n",
        "\n",
        "#TODO: Change the hyperparameters and consider custom implementations once we get something working\n",
        "transformer_decoder_layer = nn.TransformerDecoderLayer(d_model = 2 * d_model, nhead = 8)\n",
        "transformer_decoder = nn.TransformerDecoder(transformer_decoder_layer, num_layers = 6)\n",
        "\n",
        "print(transformer_decoder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSn9CPlMBcVH",
        "outputId": "a20156c0-3c7c-4f46-c9cb-1ccfc1bf9660"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TransformerDecoder(\n",
            "  (layers): ModuleList(\n",
            "    (0-5): 6 x TransformerDecoderLayer(\n",
            "      (self_attn): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1536, out_features=1536, bias=True)\n",
            "      )\n",
            "      (multihead_attn): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1536, out_features=1536, bias=True)\n",
            "      )\n",
            "      (linear1): Linear(in_features=1536, out_features=2048, bias=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (linear2): Linear(in_features=2048, out_features=1536, bias=True)\n",
            "      (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout1): Dropout(p=0.1, inplace=False)\n",
            "      (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      (dropout3): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Model"
      ],
      "metadata": {
        "id": "9eJULPtnL0r_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nj0xTETLpxtx",
        "outputId": "2cbed811-102c-4c6b-fa90-6dc861fc340d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating token: 0/156\n",
            "Generating token: 1/156\n",
            "Generating token: 2/156\n",
            "Generating token: 3/156\n",
            "Generating token: 4/156\n",
            "Generating token: 5/156\n",
            "Generating token: 6/156\n",
            "Generating token: 7/156\n",
            "Generating token: 8/156\n",
            "Generating token: 9/156\n",
            "Generating token: 10/156\n",
            "Generating token: 11/156\n",
            "Generating token: 12/156\n",
            "Generating token: 13/156\n",
            "Generating token: 14/156\n",
            "Generating token: 15/156\n",
            "Generating token: 16/156\n",
            "Generating token: 17/156\n",
            "Generating token: 18/156\n",
            "Generating token: 19/156\n",
            "Generating token: 20/156\n",
            "Generating token: 21/156\n",
            "Generating token: 22/156\n",
            "Generating token: 23/156\n",
            "Generating token: 24/156\n",
            "Generating token: 25/156\n",
            "Generating token: 26/156\n",
            "Generating token: 27/156\n",
            "Generating token: 28/156\n",
            "Generating token: 29/156\n",
            "Generating token: 30/156\n",
            "Generating token: 31/156\n",
            "Generating token: 32/156\n",
            "Generating token: 33/156\n",
            "Generating token: 34/156\n",
            "Generating token: 35/156\n",
            "Generating token: 36/156\n",
            "Generating token: 37/156\n",
            "Generating token: 38/156\n",
            "Generating token: 39/156\n",
            "Generating token: 40/156\n",
            "Generating token: 41/156\n",
            "Generating token: 42/156\n",
            "Generating token: 43/156\n",
            "Generating token: 44/156\n",
            "Generating token: 45/156\n",
            "Generating token: 46/156\n",
            "Generating token: 47/156\n",
            "Generating token: 48/156\n",
            "Generating token: 49/156\n",
            "Generating token: 50/156\n",
            "Generating token: 51/156\n",
            "Generating token: 52/156\n",
            "Generating token: 53/156\n",
            "Generating token: 54/156\n",
            "Generating token: 55/156\n",
            "Generating token: 56/156\n",
            "Generating token: 57/156\n",
            "Generating token: 58/156\n",
            "Generating token: 59/156\n",
            "Generating token: 60/156\n",
            "Generating token: 61/156\n",
            "Generating token: 62/156\n",
            "Generating token: 63/156\n",
            "Generating token: 64/156\n",
            "Generating token: 65/156\n",
            "Generating token: 66/156\n",
            "Generating token: 67/156\n",
            "Generating token: 68/156\n",
            "Generating token: 69/156\n",
            "Generating token: 70/156\n",
            "Generating token: 71/156\n",
            "Generating token: 72/156\n",
            "Generating token: 73/156\n",
            "Generating token: 74/156\n",
            "Generating token: 75/156\n",
            "Generating token: 76/156\n",
            "Generating token: 77/156\n",
            "Generating token: 78/156\n",
            "Generating token: 79/156\n",
            "Generating token: 80/156\n",
            "Generating token: 81/156\n",
            "Generating token: 82/156\n",
            "Generating token: 83/156\n",
            "Generating token: 84/156\n",
            "Generating token: 85/156\n",
            "Generating token: 86/156\n",
            "Generating token: 87/156\n",
            "Generating token: 88/156\n",
            "Generating token: 89/156\n",
            "Generating token: 90/156\n",
            "Generating token: 91/156\n",
            "Generating token: 92/156\n",
            "Generating token: 93/156\n",
            "Generating token: 94/156\n",
            "Generating token: 95/156\n",
            "Generating token: 96/156\n",
            "Generating token: 97/156\n",
            "Generating token: 98/156\n",
            "Generating token: 99/156\n",
            "Generating token: 100/156\n",
            "Generating token: 101/156\n",
            "Generating token: 102/156\n",
            "Generating token: 103/156\n",
            "Generating token: 104/156\n",
            "Generating token: 105/156\n",
            "Generating token: 106/156\n",
            "Generating token: 107/156\n",
            "Generating token: 108/156\n",
            "Generating token: 109/156\n",
            "Generating token: 110/156\n",
            "Generating token: 111/156\n",
            "Generating token: 112/156\n",
            "Generating token: 113/156\n",
            "Generating token: 114/156\n",
            "Generating token: 115/156\n",
            "Generating token: 116/156\n",
            "Generating token: 117/156\n",
            "Generating token: 118/156\n",
            "Generating token: 119/156\n",
            "Generating token: 120/156\n",
            "Generating token: 121/156\n",
            "Generating token: 122/156\n",
            "Generating token: 123/156\n",
            "Generating token: 124/156\n",
            "Generating token: 125/156\n",
            "Generating token: 126/156\n",
            "Generating token: 127/156\n",
            "Generating token: 128/156\n",
            "Generating token: 129/156\n",
            "Generating token: 130/156\n",
            "Generating token: 131/156\n",
            "Generating token: 132/156\n",
            "Generating token: 133/156\n",
            "Generating token: 134/156\n",
            "Generating token: 135/156\n",
            "Generating token: 136/156\n",
            "Generating token: 137/156\n",
            "Generating token: 138/156\n",
            "Generating token: 139/156\n",
            "Generating token: 140/156\n",
            "Generating token: 141/156\n",
            "Generating token: 142/156\n",
            "Generating token: 143/156\n",
            "Generating token: 144/156\n",
            "Generating token: 145/156\n",
            "Generating token: 146/156\n",
            "Generating token: 147/156\n",
            "Generating token: 148/156\n",
            "Generating token: 149/156\n",
            "Generating token: 150/156\n",
            "Generating token: 151/156\n",
            "Generating token: 152/156\n",
            "Generating token: 153/156\n",
            "Generating token: 154/156\n",
            "Generating token: 155/156\n",
            "Generating token: 156/156\n",
            "Generated tokens: [tensor([[ 1.5441,  0.2463, -0.8940,  ...,  0.4309, -0.3251, -1.3998]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 1.1030, -0.0884, -0.8996,  ...,  0.2934,  0.4717, -1.3582]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 1.4233, -0.1947, -0.6605,  ...,  0.7392,  1.0170, -0.8824]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 1.5273,  0.0959, -0.5146,  ...,  1.3760, -0.2358, -1.0743]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 1.5194,  0.5175, -0.3203,  ...,  1.8184, -0.8702, -1.4725]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 1.3374,  0.6422, -0.4210,  ...,  1.1710, -0.6039, -1.1098]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.9293,  0.7074, -0.6234,  ...,  0.2564, -0.7433, -0.7955]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 1.0353,  0.0403, -0.5898,  ..., -0.0169, -0.6534, -0.5524]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 1.5412,  0.0491, -0.4561,  ...,  0.7277, -0.4273, -0.9282]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 1.4147, -0.0792, -0.4651,  ...,  0.6544, -0.7243, -0.8258]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 1.2730, -0.5829, -1.2211,  ...,  0.8420, -0.1373, -0.3902]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.6642, -0.0953, -0.9659,  ..., -0.1587,  0.7284, -0.3378]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.7979,  0.1099, -0.9238,  ..., -0.2115,  0.3646, -0.2239]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.8483,  0.0395, -0.5268,  ...,  0.8545, -0.2438, -0.1980]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.6001, -0.6868, -0.3108,  ...,  0.7318,  0.1891, -1.1042]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.5690, -0.6195, -0.4736,  ...,  0.5280, -0.0591, -1.4293]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.6309, -0.7226, -0.6146,  ...,  0.7556, -0.3485, -1.7841]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.4697, -0.5470, -0.3186,  ...,  0.3545,  0.1899, -1.4626]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.4129, -0.1658, -0.4044,  ...,  0.3479, -0.4360, -1.3820]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 1.2301, -0.9499, -0.3177,  ...,  0.8684,  0.0256, -1.3664]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.7932, -0.3063, -0.7716,  ..., -0.3683,  0.0525, -1.2294]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.9233, -0.5266, -0.1488,  ...,  0.8626, -0.1785, -1.1845]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.9207, -1.0716, -0.5345,  ...,  1.2675, -0.0089, -0.9887]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.6975, -0.0718, -0.3832,  ...,  1.6572, -0.6748, -0.2564]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 1.1858, -0.2117,  0.0922,  ...,  1.8004, -0.1782, -0.2282]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 1.2710, -0.5763, -0.2683,  ...,  1.4234,  0.6232, -0.6522]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.9328, -0.0099,  0.0810,  ...,  1.0618,  0.6959, -1.0408]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.7333, -0.8188,  0.1466,  ...,  1.1340,  0.3893, -0.8298]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.8950, -0.4145, -0.2401,  ...,  1.1755,  0.4992, -0.8391]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.3543, -0.6721, -0.7901,  ...,  0.7486,  0.3436, -0.9623]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.6998, -0.6772, -0.1070,  ...,  1.5896, -0.1068, -0.7049]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.8308, -0.8412,  0.0954,  ...,  1.2057, -0.1813, -1.6964]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 1.4835, -0.6291, -0.0515,  ...,  0.8989, -0.3232, -1.4061]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.7731, -0.5318, -0.6185,  ...,  0.6377,  0.4402, -1.3283]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.5366, -0.4308, -0.3994,  ...,  1.1672,  0.3751, -1.4465]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.3920, -0.7064, -0.4126,  ...,  0.7298,  0.8647, -0.8208]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.6082, -0.8232, -0.6401,  ...,  0.9170,  0.1726, -0.7889]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.2874, -0.5225, -0.6422,  ...,  0.8035,  0.1395, -0.8329]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 1.1056, -0.1213, -0.5445,  ...,  0.9943,  0.2832, -1.0634]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.9937, -0.3223, -0.8639,  ...,  1.0186,  0.4232, -1.9983]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.9116, -0.0581, -0.7248,  ...,  0.9391,  0.5614, -0.8346]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.9689,  0.0126, -0.5973,  ...,  1.0086,  0.2505, -0.9213]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.4443, -0.2297, -0.9025,  ...,  1.2317,  0.1365, -1.5037]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.1777, -0.3546, -0.9652,  ...,  0.5361,  0.0398, -1.2178]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[-0.0441, -0.2358, -0.7768,  ...,  1.2047, -0.0431, -0.6295]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.1377, -0.6303, -0.2199,  ...,  1.5182, -0.6616, -0.6914]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.4090, -0.4969, -0.5376,  ...,  1.3393, -0.3555, -0.4091]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.1584,  0.3099, -0.2081,  ...,  1.0371,  0.6212, -0.6209]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.3725, -0.4721, -0.2606,  ...,  1.4394,  0.8806, -0.9220]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[-0.1655, -0.8406, -0.0654,  ...,  1.5598,  0.0303, -0.9779]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 2.8105e-01, -3.9070e-01, -8.7208e-04,  ...,  7.9004e-01,\n",
            "         -1.3027e-01, -1.0518e+00]], grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.5778, -0.6496,  0.1051,  ...,  1.5302,  0.1962, -1.2544]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.4781, -0.7112, -0.4100,  ...,  1.1591,  0.8123, -0.5778]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.6065, -0.6655, -0.5989,  ...,  0.9421,  0.2942, -0.6965]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.9827, -0.9622,  0.0537,  ...,  1.5180, -0.1199, -0.9542]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.8360, -0.7018, -0.0878,  ...,  0.9458, -0.0348, -0.9705]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 1.2526, -0.0946, -0.2112,  ...,  0.8563,  0.2624, -0.3029]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 1.1149, -0.1073, -0.0770,  ...,  0.9707,  0.5052, -0.4633]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 1.0136,  0.1409, -0.4870,  ...,  0.6311, -0.0823, -0.4193]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.9597,  0.0968, -0.6969,  ...,  1.2868, -0.2091, -1.0303]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.7719,  0.0691, -0.6115,  ...,  1.6968,  0.2724, -1.1186]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.6873,  0.1870, -0.9432,  ...,  1.3969,  0.3628, -0.0018]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.4304,  0.3937, -1.1550,  ...,  1.4746,  0.2600, -0.0998]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.3829,  0.0403, -0.5600,  ...,  1.5544,  0.5768, -0.6681]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.6750, -0.2983, -0.3492,  ...,  1.4625,  0.2940, -0.9302]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.1062,  0.1772, -0.5475,  ...,  0.9459,  0.8753, -0.4743]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.7095,  0.1370, -0.1790,  ...,  1.2638,  0.1964, -0.3378]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.3957, -0.5466, -0.1116,  ...,  0.8331,  0.7744, -1.5575]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.3598, -0.4585, -0.1626,  ...,  0.7799,  0.1642, -1.5927]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[-0.2977, -1.0318, -0.0120,  ...,  0.5697, -0.3006, -1.2571]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.0268, -1.2310, -0.4060,  ...,  1.1592,  0.8772, -1.2124]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.3082, -1.1163,  0.1773,  ...,  1.1897,  0.1226, -1.8590]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[-0.3090, -0.5114, -0.6528,  ...,  1.2427,  0.4149, -1.2511]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.4501, -0.5694, -0.4669,  ...,  1.4680,  0.3492, -1.4624]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.6529, -0.2540, -0.5796,  ...,  0.4916,  0.0957, -1.7745]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.4794, -0.9519, -0.9197,  ...,  0.3771,  0.5653, -1.9769]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.5748, -1.3308, -0.3477,  ...,  0.8344,  0.3496, -1.2008]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 1.0053, -0.4344, -0.9422,  ...,  0.8065, -0.0746, -1.1941]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.3018, -0.4511, -1.2186,  ...,  1.1917,  0.5476, -1.3186]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.5969, -0.4251, -0.6484,  ...,  1.2605,  0.5119, -1.3196]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.1589, -0.3487,  0.3493,  ...,  0.5470,  0.7070, -1.3577]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[-3.2156e-02, -1.6821e-01,  2.9350e-01,  ...,  1.0925e+00,\n",
            "          6.3607e-04, -1.5391e+00]], grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.4172, -0.0548,  0.6616,  ...,  1.0906, -0.3766, -1.6007]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.1236,  0.6136,  0.1382,  ...,  0.8955,  0.0355, -1.3055]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[-0.0854,  0.1426, -0.4742,  ...,  1.2197, -0.0283, -1.1469]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.5434,  0.4620, -0.6269,  ...,  1.5269,  0.6176, -1.2126]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 1.1672,  0.0608, -0.6079,  ...,  1.9882, -0.5090, -1.0980]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.7157,  0.4681, -0.6677,  ...,  0.9548, -0.1918, -1.2039]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.6758,  0.2044, -0.5678,  ...,  0.7117, -0.2207, -0.2899]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.9234, -0.3528, -0.5497,  ...,  0.4760, -0.2841, -0.7436]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.7679, -0.0498, -0.1087,  ...,  0.7492, -0.3348, -0.7557]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 1.3686, -0.4100, -0.0707,  ...,  1.0500, -0.0575, -0.6488]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 1.0017, -0.8317, -0.3089,  ...,  1.5338,  0.0421, -1.2059]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.4587, -0.7162, -0.4749,  ...,  0.7359,  0.0977, -1.4795]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.9030, -0.8046,  0.0940,  ..., -0.1509,  0.4677, -0.2910]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.5918, -0.7283, -0.2556,  ..., -0.0927, -0.0479, -0.2437]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.9365, -0.5848, -0.0623,  ...,  0.8741, -0.2462, -0.4681]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.9112,  0.0259, -0.1947,  ...,  1.0431,  0.2674, -1.1221]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.5469,  0.0555, -0.0112,  ...,  1.2169,  0.1900, -1.5756]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.4427, -0.0572, -0.8960,  ...,  0.4955, -0.1924, -1.6418]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.1826, -0.3027, -0.1473,  ...,  1.1250, -0.8096, -1.3910]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.4054, -1.3390, -0.2608,  ...,  0.8547, -0.1151, -0.9574]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.7058, -0.9110, -0.5314,  ...,  1.4399,  0.6514, -0.6023]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.6285, -0.2099, -0.3872,  ...,  0.8439,  0.0337, -1.0331]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.4556, -0.2308, -0.7971,  ...,  0.8851,  0.3727, -0.2793]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 1.1640, -0.1359, -0.3779,  ...,  1.3713,  0.2830, -0.3970]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 1.6541,  0.3748, -1.1819,  ...,  0.9143,  0.8861, -0.9178]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 1.7742,  0.4930, -0.4629,  ...,  1.2831,  0.4273, -0.2000]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 1.3701,  0.0342, -0.4640,  ...,  1.7545, -1.0815, -0.4630]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.7682,  0.5264, -0.6646,  ...,  1.6562, -0.7355, -0.8852]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.4864,  0.9084, -0.4733,  ...,  1.4666, -0.0350, -0.6755]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.3530, -0.3549, -0.3321,  ...,  0.3813, -0.0155, -0.2004]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.8614, -0.7243, -0.3636,  ...,  0.2397,  0.2800,  0.0359]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.2684,  0.1682, -0.0319,  ...,  0.5456, -0.0386, -0.3446]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.7701,  0.1768,  0.2410,  ...,  0.5272,  0.1458, -0.3628]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.8634,  0.0309,  0.6744,  ...,  0.9379, -0.1577, -0.5246]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.7367, -0.0858,  0.6397,  ...,  0.8176, -0.1884, -1.2536]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.1022, -0.2109, -0.4375,  ...,  0.6964, -0.3985, -1.4558]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[-0.3320, -0.8017, -0.5177,  ...,  0.6037, -0.3595, -0.6889]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[-0.1806, -1.1655, -0.0056,  ...,  0.5586,  0.0295, -0.6837]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.8929, -0.9904,  0.2013,  ...,  0.8010,  0.2626, -1.1489]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.8917, -0.5935, -0.2316,  ...,  1.3288,  0.3670, -1.0255]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 1.2955, -0.6763,  0.0219,  ...,  1.2028, -0.1912, -0.7739]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.5612, -0.4481, -0.5099,  ...,  0.9627, -0.0683, -1.3220]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.5225, -0.1956, -0.5549,  ...,  0.3178,  0.1821, -0.7783]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.7725, -0.6132,  0.2552,  ...,  1.0081, -0.2587, -0.5894]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 1.1513, -0.7259, -0.2877,  ...,  0.9908, -0.3794, -0.5510]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.9770, -0.0185, -0.1271,  ...,  1.7238,  0.0050, -0.8343]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 1.3745, -0.2108, -0.2132,  ...,  1.4128, -0.5929, -0.6889]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.5653, -0.2731, -0.6235,  ...,  0.6004, -0.1480, -1.1645]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 1.3298, -0.6677, -0.4488,  ...,  0.8403, -0.8848, -1.2699]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.7594,  0.3128, -1.0136,  ...,  1.1678, -0.4372, -0.3822]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 1.2483, -0.2368, -0.8434,  ...,  0.9647, -0.9283, -0.3140]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 1.3744, -0.0151, -0.2012,  ...,  0.8737,  0.1632, -0.9406]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 1.0618, -0.1367, -0.4360,  ...,  0.5519,  0.4466, -0.8850]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 1.0904, -0.3821, -0.1738,  ...,  0.7925, -0.0541, -0.5444]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 1.0313,  0.1611, -0.4448,  ...,  1.0861, -0.5228, -0.5868]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.7049, -0.3501,  0.0194,  ...,  0.9077,  0.8271, -0.4566]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.5110,  0.0035, -0.3707,  ...,  1.6734,  0.8040, -0.3456]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.4612, -0.4825, -0.5204,  ...,  1.3800,  0.6762, -0.6716]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.7857, -0.3309, -1.0191,  ...,  0.9185,  0.4097, -0.8740]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.3214,  0.4050, -0.5739,  ...,  1.5790,  0.3126, -0.7491]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.5315, -0.4836, -0.7087,  ...,  1.6062,  0.7720, -0.9256]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.2581, -0.0519, -1.3558,  ...,  0.9567,  0.4411, -0.7983]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.7047,  0.4499, -0.9438,  ...,  0.5965,  0.5496, -1.1176]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.6047, -0.5052, -0.5438,  ...,  0.7963,  0.3752, -1.0276]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.2168,  0.0995,  0.2861,  ..., -0.2808, -0.3625, -0.7674]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.5401,  0.1474,  0.3380,  ...,  0.5598, -0.7173, -0.3808]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.5559,  0.0649, -0.4771,  ...,  1.1960,  0.0578, -0.2773]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[-0.0142, -0.4930, -0.2833,  ...,  0.5597, -0.0095, -0.8485]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[-0.3225,  0.5398, -1.2342,  ...,  0.9546,  0.0806, -0.8715]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.1711, -0.0738, -0.1844,  ...,  1.3809, -0.1077, -0.2931]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[-0.2751, -0.7132, -0.2794,  ...,  1.4167, -0.4640, -0.8150]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[-0.1537, -0.7399, -0.6342,  ...,  0.5822,  0.2828, -0.8127]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.2848, -0.7484, -0.3767,  ...,  1.2376,  0.2107, -0.9637]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.6561, -1.2920, -0.4189,  ...,  0.2333,  0.3009, -1.2467]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[ 0.0773, -0.6079,  0.2883,  ...,  1.1038, -0.2024, -0.9892]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)]\n"
          ]
        }
      ],
      "source": [
        "# TODO: Just taking the last layer for fine grain acoustic tokens and middle layer for semantic tokens, perhaps this should be tuned?\n",
        "src_fine_grain_tokens = src_hidden_states[-1]\n",
        "src_semantic_tokens = src_hidden_states[int(src_hidden_states.size(dim = 0) / 2)]\n",
        "\n",
        "# TODO: Is just concatenating the fine grain tokens and semantic tokens acceptable?\n",
        "# Ideally the dimension here needs to match with the eventual encoded dimensions\n",
        "src_hybrid_tokens = torch.cat((src_fine_grain_tokens, src_semantic_tokens), dim=1)\n",
        "\n",
        "# TODO: What is the appropriate starting token?\n",
        "previously_generated_token = torch.zeros([1, 2 * d_model])\n",
        "\n",
        "generated_tokens = []\n",
        "\n",
        "for index, encodec_token in enumerate(encodec_tokens[1]):\n",
        "  print(\"Generating token: \" + str(index) + \"/\" + str(seq_length))\n",
        "\n",
        "  generated_token = transformer_decoder(tgt = previously_generated_token, memory = src_hybrid_tokens)\n",
        "  #TODO: Calculate loss between generated_token and encodec_token and then backpropogate once per loop?\n",
        "\n",
        "  generated_tokens.append(generated_token)\n",
        "  previously_generated_token = generated_token\n",
        "\n",
        "print(\"Generated tokens: \" + str(generated_tokens))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RSPihxrStIi3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}