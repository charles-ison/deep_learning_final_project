{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -qqq install transformers datasets nnAudio"
      ],
      "metadata": {
        "id": "Dl19Asqk79-G"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "KFbW0b6v7uAX"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2FeatureExtractor\n",
        "from transformers import AutoModel\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchaudio.transforms as T\n",
        "from datasets import Dataset, Audio, concatenate_datasets, Split\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mount drive and set path to dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_dir = \"/content/drive/Shareddrives/DeepLearningProject/minibabyslakh\"\n",
        "# make sure \n",
        "os.listdir(data_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXOmbPCXPgTP",
        "outputId": "8c3b9082-14b9-4f31-b774-9bfab176fb87"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['train', 'test']"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading our model weights\n",
        "model = AutoModel.from_pretrained(\"m-a-p/MERT-v0\", trust_remote_code=True)\n",
        "# loading the corresponding preprocessor config\n",
        "processor = Wav2Vec2FeatureExtractor.from_pretrained(\"m-a-p/MERT-v0\",trust_remote_code=True)"
      ],
      "metadata": {
        "id": "pxhU5aIY8KRL"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # load demo audio and set processor\n",
        "# dataset = Dataset.load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n",
        "# dataset = dataset.sort(\"id\")\n",
        "# sampling_rate = dataset.features[\"audio\"].sampling_rate"
      ],
      "metadata": {
        "id": "guN3LtRXYhuk"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load the audio files from the directory structure\n",
        "def get_data_files(directory):\n",
        "    bass_files = []\n",
        "    residual_files = []\n",
        "    tracks = []\n",
        "    for track_dir in os.listdir(directory):\n",
        "        track_path = os.path.join(directory, track_dir)\n",
        "        if os.path.isdir(track_path):\n",
        "            bass_audio_dir = os.path.join(track_path, 'bass')\n",
        "            # bass_file = os.path.join(bass_audio_dir, 'bass.wav')\n",
        "            # residual_file = os.path.join(bass_audio_dir, 'residuals.wav')\n",
        "            if os.path.isdir(bass_audio_dir):\n",
        "                for file in os.listdir(bass_audio_dir):\n",
        "                    if file.startswith('bass') and file.endswith('.wav'):\n",
        "                        bass_file = os.path.join(bass_audio_dir, file)\n",
        "                        bass_files.append(bass_file)\n",
        "                        residual_file = os.path.join(bass_audio_dir, 'residuals' + file[4:])\n",
        "                        residual_files.append(residual_file)\n",
        "                        tracks.append(track_dir)\n",
        "        \n",
        "    return {\"bass\": bass_files, \"residuals\": residual_files, \"track\": tracks}"
      ],
      "metadata": {
        "id": "-SCGvvx9JHtw"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the audio filenames from the dataset directory\n",
        "train_files = get_data_files(os.path.join(data_dir, \"train\"))\n",
        "test_files = get_data_files(os.path.join(data_dir, \"test\"))\n",
        "# validation_data = load_audio_files(os.path.join(data_dir, \"validation\"))\n",
        "train_files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0rXcxIlS1yK",
        "outputId": "fdd717d4-6cdb-46cd-8262-0fc08918928f"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bass': ['/content/drive/Shareddrives/DeepLearningProject/minibabyslakh/train/Track00002/bass/bass.wav',\n",
              "  '/content/drive/Shareddrives/DeepLearningProject/minibabyslakh/train/Track00001/bass/bass.wav',\n",
              "  '/content/drive/Shareddrives/DeepLearningProject/minibabyslakh/train/Track00003/bass/bass.wav'],\n",
              " 'residuals': ['/content/drive/Shareddrives/DeepLearningProject/minibabyslakh/train/Track00002/bass/residuals.wav',\n",
              "  '/content/drive/Shareddrives/DeepLearningProject/minibabyslakh/train/Track00001/bass/residuals.wav',\n",
              "  '/content/drive/Shareddrives/DeepLearningProject/minibabyslakh/train/Track00003/bass/residuals.wav'],\n",
              " 'track': ['Track00002', 'Track00001', 'Track00003']}"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the dataset objects\n",
        "train_dataset = Dataset.from_dict(train_files, split=\"train\") \\\n",
        "                    .cast_column(\"bass\", Audio()) \\\n",
        "                    .cast_column(\"residuals\", Audio()) \\\n",
        "                    .sort(\"track\")\n",
        "test_dataset = Dataset.from_dict(test_files, split=\"test\") \\\n",
        "                    .cast_column(\"bass\", Audio()) \\\n",
        "                    .cast_column(\"residuals\", Audio()) \\\n",
        "                    .sort(\"track\")\n",
        "combined_dataset = concatenate_datasets([train_dataset, test_dataset])\n",
        "\n",
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4tfjuUfYpCV",
        "outputId": "b4de0eaa-a6f0-4f91-b26e-fdd72d7e7771"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['bass', 'residuals', 'track'],\n",
              "    num_rows: 3\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampling_rate = train_dataset[\"residuals\"][0]['sampling_rate']\n",
        "resample_rate = processor.sampling_rate\n",
        "# make sure the sample_rate aligned\n",
        "if resample_rate != sampling_rate:\n",
        "    print(f'setting rate from {sampling_rate} to {resample_rate}')\n",
        "    resampler = T.Resample(sampling_rate, resample_rate)\n",
        "else:\n",
        "    resampler = None"
      ],
      "metadata": {
        "id": "3kxvhTbB-IKA"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# audio file is decoded on the fly\n",
        "if resampler is None:\n",
        "    input_audio = train_dataset[0][\"residuals\"][\"array\"]\n",
        "else:\n",
        "  input_audio = resampler(torch.from_numpy(train_dataset[0][\"residuals\"][\"array\"]))"
      ],
      "metadata": {
        "id": "KIn1GRQa-wx-"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The whole audio file is too big to run in colab\n",
        "input_audio = input_audio[0:93680]"
      ],
      "metadata": {
        "id": "cjrxz_BYiYiN"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = processor(input_audio, sampling_rate=resample_rate, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs, output_hidden_states=True)"
      ],
      "metadata": {
        "id": "phYaHnl-_BRc"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take a look at the output shape, there are 13 layers of representation\n",
        "# each layer performs differently in different downstream tasks, you should choose empirically\n",
        "all_layer_hidden_states = torch.stack(outputs.hidden_states).squeeze()\n",
        "print(all_layer_hidden_states.shape) # [13 layer, Time steps, 768 feature_dim]\n",
        "\n",
        "# for utterance level classification tasks, you can simply reduce the representation in time\n",
        "time_reduced_hidden_states = all_layer_hidden_states.mean(-2)\n",
        "print(time_reduced_hidden_states.shape) # [13, 768]\n",
        "\n",
        "# you can even use a learnable weighted average representation\n",
        "aggregator = nn.Conv1d(in_channels=13, out_channels=1, kernel_size=1)\n",
        "weighted_avg_hidden_states = aggregator(time_reduced_hidden_states.unsqueeze(0)).squeeze()\n",
        "print(weighted_avg_hidden_states.shape) # [768]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhaB1K8-iJXR",
        "outputId": "1f617c1b-1164-4c38-b119-cdb89309ba45"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([13, 292, 768])\n",
            "torch.Size([13, 768])\n",
            "torch.Size([768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nj0xTETLpxtx",
        "outputId": "67696c3a-39c7-45e2-c0ab-a16a808b0370"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BaseModelOutput(last_hidden_state=tensor([[[-0.0606,  0.1180,  0.0528,  ..., -0.0023,  0.0381,  0.1199],\n",
              "         [-0.0716,  0.1062,  0.0683,  ..., -0.0027,  0.0267,  0.1263],\n",
              "         [-0.0757,  0.1017,  0.0629,  ..., -0.0036,  0.0363,  0.1262],\n",
              "         ...,\n",
              "         [-0.0952,  0.1915,  0.0160,  ...,  0.0156,  0.0945,  0.2356],\n",
              "         [ 0.0911,  0.1736,  0.0869,  ...,  0.0308, -0.0166,  0.3092],\n",
              "         [ 0.0073,  0.3786,  0.0152,  ..., -0.0048, -0.2137,  0.3118]]]), hidden_states=(tensor([[[-0.1507, -0.4730,  0.0677,  ...,  0.2829, -0.1228, -0.1929],\n",
              "         [-0.1839, -0.4138,  0.0656,  ...,  0.2732, -0.1123, -0.1129],\n",
              "         [-0.2044, -0.3500,  0.0509,  ...,  0.2640, -0.1125, -0.0760],\n",
              "         ...,\n",
              "         [ 0.5956, -0.1389, -0.1069,  ...,  0.3431, -0.3568,  0.1605],\n",
              "         [ 0.2148,  0.0763, -0.1094,  ...,  0.2505, -0.2648,  0.1472],\n",
              "         [-0.0154,  0.3685, -0.1005,  ...,  0.2217, -0.1888, -0.0043]]]), tensor([[[-0.0886, -0.2003,  0.1322,  ...,  0.0872, -0.4373, -0.2603],\n",
              "         [-0.0553, -0.1670,  0.1238,  ...,  0.0735, -0.4636, -0.1707],\n",
              "         [-0.0557, -0.1420,  0.0616,  ...,  0.0887, -0.4513, -0.1102],\n",
              "         ...,\n",
              "         [ 0.4655, -0.0240, -0.2454,  ...,  0.1303, -0.5713,  0.1564],\n",
              "         [ 0.1674,  0.0187, -0.2329,  ...,  0.0843, -0.4377,  0.0606],\n",
              "         [-0.0866,  0.1221, -0.3024,  ...,  0.1229, -0.4124, -0.0992]]]), tensor([[[-0.0508, -0.0913,  0.0299,  ..., -0.1736, -0.4211, -0.0819],\n",
              "         [ 0.0069, -0.0691,  0.0652,  ..., -0.1818, -0.5281,  0.0064],\n",
              "         [ 0.0259, -0.0628, -0.0044,  ..., -0.1739, -0.5466,  0.0594],\n",
              "         ...,\n",
              "         [ 0.1857,  0.0693, -0.4241,  ...,  0.0386, -0.5140,  0.2668],\n",
              "         [ 0.0589, -0.0074, -0.3943,  ..., -0.0274, -0.5015,  0.1435],\n",
              "         [-0.1137, -0.0783, -0.6524,  ...,  0.0697, -0.5766,  0.1125]]]), tensor([[[-0.0253,  0.0200,  0.0889,  ...,  0.0141, -0.1887, -0.5172],\n",
              "         [ 0.0775,  0.0596,  0.2030,  ..., -0.0224, -0.3222, -0.3711],\n",
              "         [ 0.0677,  0.0877,  0.1634,  ..., -0.0457, -0.3454, -0.2878],\n",
              "         ...,\n",
              "         [ 0.1244,  0.0651, -0.2791,  ...,  0.1746, -0.3503,  0.1768],\n",
              "         [ 0.1588,  0.0388, -0.1592,  ...,  0.0571, -0.2433,  0.1155],\n",
              "         [ 0.0818,  0.0646, -0.5193,  ...,  0.1638, -0.2073, -0.0353]]]), tensor([[[ 0.0827,  0.0406,  0.2884,  ..., -0.0945, -0.0013, -0.2532],\n",
              "         [ 0.2229,  0.0278,  0.3358,  ..., -0.1191, -0.0838, -0.0824],\n",
              "         [ 0.2686,  0.0284,  0.2811,  ..., -0.1220, -0.1004,  0.0544],\n",
              "         ...,\n",
              "         [-0.1726,  0.1582, -0.1331,  ..., -0.0167, -0.0170,  0.1257],\n",
              "         [-0.0823,  0.1103, -0.0560,  ..., -0.0390,  0.0736,  0.1945],\n",
              "         [-0.1351,  0.1082, -0.5564,  ..., -0.1177,  0.1451,  0.1903]]]), tensor([[[-0.0209, -0.0377,  0.4397,  ...,  0.1544,  0.7061,  0.1184],\n",
              "         [ 0.1617,  0.0349,  0.4110,  ...,  0.1792,  0.6615,  0.2975],\n",
              "         [ 0.1820,  0.0668,  0.3647,  ...,  0.1745,  0.6345,  0.3987],\n",
              "         ...,\n",
              "         [-0.1177,  0.0490,  0.1408,  ...,  0.1199,  0.0109,  0.1632],\n",
              "         [-0.0173,  0.0171,  0.2030,  ...,  0.0847,  0.1497,  0.3363],\n",
              "         [-0.0606,  0.0161, -0.1663,  ...,  0.0488,  0.1472,  0.2574]]]), tensor([[[ 0.1259,  0.0718,  0.5595,  ...,  0.2316,  0.2177,  0.0220],\n",
              "         [ 0.2207,  0.0625,  0.5039,  ...,  0.1986,  0.1563,  0.1527],\n",
              "         [ 0.2322,  0.0576,  0.4526,  ...,  0.2050,  0.1364,  0.2045],\n",
              "         ...,\n",
              "         [-0.1583,  0.0182,  0.1529,  ...,  0.0139, -0.1156,  0.1566],\n",
              "         [-0.1577,  0.0667,  0.2834,  ...,  0.0234,  0.0667,  0.3213],\n",
              "         [-0.2374,  0.0028,  0.0566,  ..., -0.0210, -0.0209,  0.3312]]]), tensor([[[ 0.2698,  0.0131,  0.8971,  ...,  0.0338,  0.5516,  0.0036],\n",
              "         [ 0.3873,  0.0208,  0.8544,  ..., -0.0032,  0.4991,  0.0513],\n",
              "         [ 0.4184,  0.0160,  0.8045,  ..., -0.0112,  0.4966,  0.0535],\n",
              "         ...,\n",
              "         [ 0.0394,  0.0029,  0.3853,  ...,  0.0489, -0.1946,  0.1307],\n",
              "         [ 0.1855, -0.0221,  0.4457,  ..., -0.0571, -0.0104,  0.2934],\n",
              "         [ 0.0073, -0.0451,  0.2001,  ..., -0.0931, -0.0872,  0.3672]]]), tensor([[[-0.0446,  0.0233,  0.4184,  ..., -0.0509,  0.3902,  0.1753],\n",
              "         [ 0.0600,  0.1022,  0.2850,  ..., -0.0513,  0.2585,  0.2082],\n",
              "         [ 0.1121,  0.1441,  0.2075,  ..., -0.0460,  0.2011,  0.2181],\n",
              "         ...,\n",
              "         [-0.0116,  0.4902,  0.6131,  ..., -0.1941,  0.1084, -0.1724],\n",
              "         [ 0.1892,  0.4383,  0.5448,  ..., -0.2369,  0.2610,  0.0798],\n",
              "         [ 0.1370,  0.2647,  0.1134,  ..., -0.3056,  0.0160,  0.2708]]]), tensor([[[-0.2810,  0.0437,  0.2058,  ..., -0.1332,  0.7067,  0.2454],\n",
              "         [-0.1366,  0.0837,  0.1659,  ..., -0.1142,  0.6170,  0.3269],\n",
              "         [-0.0807,  0.1181,  0.1458,  ..., -0.1048,  0.5707,  0.3345],\n",
              "         ...,\n",
              "         [-0.0318,  0.2105,  0.4764,  ..., -0.1495,  0.4577,  0.2542],\n",
              "         [ 0.1235,  0.1034,  0.4715,  ..., -0.2617,  0.5054,  0.3532],\n",
              "         [-0.3439, -0.0394,  0.2056,  ..., -0.2293, -0.0357,  0.1728]]]), tensor([[[-0.1349,  0.0032, -0.0236,  ...,  0.0099,  0.5796,  0.3995],\n",
              "         [-0.0678,  0.0542, -0.0303,  ...,  0.0131,  0.5395,  0.3589],\n",
              "         [-0.0230,  0.0556, -0.0401,  ..., -0.0025,  0.5194,  0.3199],\n",
              "         ...,\n",
              "         [-0.0787,  0.3201, -0.0308,  ..., -0.3915,  0.3517,  0.4037],\n",
              "         [ 0.2214,  0.2887, -0.0733,  ..., -0.3979,  0.2858,  0.3733],\n",
              "         [-0.2437,  0.2901,  0.0710,  ..., -0.1715, -0.2243,  0.2192]]]), tensor([[[-0.2081, -0.1121,  0.0296,  ...,  0.0500,  0.2414,  0.3206],\n",
              "         [-0.2259, -0.0993,  0.0484,  ...,  0.0332,  0.2109,  0.3084],\n",
              "         [-0.2216, -0.0990,  0.0427,  ...,  0.0169,  0.1945,  0.2857],\n",
              "         ...,\n",
              "         [-0.1911,  0.0968,  0.0342,  ..., -0.0826,  0.3938,  0.3282],\n",
              "         [-0.0522,  0.1415,  0.0234,  ..., -0.0892,  0.1901,  0.3757],\n",
              "         [-0.1983,  0.1710,  0.1570,  ..., -0.1182, -0.0181,  0.1968]]]), tensor([[[-0.0606,  0.1180,  0.0528,  ..., -0.0023,  0.0381,  0.1199],\n",
              "         [-0.0716,  0.1062,  0.0683,  ..., -0.0027,  0.0267,  0.1263],\n",
              "         [-0.0757,  0.1017,  0.0629,  ..., -0.0036,  0.0363,  0.1262],\n",
              "         ...,\n",
              "         [-0.0952,  0.1915,  0.0160,  ...,  0.0156,  0.0945,  0.2356],\n",
              "         [ 0.0911,  0.1736,  0.0869,  ...,  0.0308, -0.0166,  0.3092],\n",
              "         [ 0.0073,  0.3786,  0.0152,  ..., -0.0048, -0.2137,  0.3118]]])), attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    }
  ]
}